flask
from fastapi import FastAPI, File, UploadFile, HTTPException
from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig
import os
import uuid

app = FastAPI()

# Replace these with your actual Azure subscription key and region
subscription_key = "YourAzureSubscriptionKey"
region = "YourAzureRegion"  # e.g., "eastus"

# Directory to temporarily store the uploaded audio file
audio_folder = "uploads/"
if not os.path.exists(audio_folder):
    os.makedirs(audio_folder)

@app.post("/recognize-speech/")
async def recognize_speech(file: UploadFile = File(...)):
    try:
        # Generate a unique filename for the uploaded file
        file_name = str(uuid.uuid4()) + ".wav"
        file_path = os.path.join(audio_folder, file_name)

        # Save the uploaded file to the disk temporarily
        with open(file_path, "wb") as f:
            f.write(await file.read())

        # Initialize speech configuration with Azure subscription key and region
        speech_config = SpeechConfig(subscription=subscription_key, region=region)
        
        # Create AudioConfig pointing to the saved audio file
        audio_config = AudioConfig(filename=file_path)
        
        # Create SpeechRecognizer object
        recognizer = SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
        
        # Start speech recognition and get the result
        result = recognizer.recognize_once()
        
        # Check if speech was recognized successfully
        if result.reason == result.Reason.RecognizedSpeech:
            return {"recognized_text": result.text}
        else:
            raise HTTPException(status_code=500, detail="Speech recognition failed")

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



---------
import { Injectable } from '@angular/core';
import { HttpClient } from '@angular/common/http';
import { Observable } from 'rxjs';

@Injectable({
  providedIn: 'root',
})
export class SpeechToTextService {
  private baseUrl = 'http://localhost:8000'; // FastAPI backend on localhost

  constructor(private http: HttpClient) {}

  // Method to send the audio file to FastAPI for transcription
  transcribeSpeech(file: Blob): Observable<any> {
    const formData = new FormData();
    formData.append('file', file, 'audio.wav'); // Append the audio file to FormData

    return this.http.post<any>(`${this.baseUrl}/recognize-speech/`, formData);
  }
}
-------

import { Component } from '@angular/core';
import { SpeechToTextService } from './speech-to-text.service';

@Component({
  selector: 'app-root',
  templateUrl: './app.component.html',
  styleUrls: ['./app.component.css'],
})
export class AppComponent {
  recognizedText: string = '';
  isRecording = false;
  mediaRecorder: MediaRecorder | null = null;
  audioChunks: Blob[] = [];

  constructor(private speechToTextService: SpeechToTextService) {}

  // Start recording
  startRecording() {
    this.isRecording = true;
    this.audioChunks = [];

    if (navigator.mediaDevices) {
      navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {
        this.mediaRecorder = new MediaRecorder(stream);

        // When audio data is available, push it to the audioChunks array
        this.mediaRecorder.ondataavailable = (event) => {
          this.audioChunks.push(event.data);
        };

        // When the recording stops, combine the chunks and send the audio file to the backend
        this.mediaRecorder.onstop = () => {
          const audioBlob = new Blob(this.audioChunks, { type: 'audio/wav' });
          this.sendAudioToBackend(audioBlob);
        };

        this.mediaRecorder.start();
      });
    } else {
      console.error('Audio recording not supported.');
    }
  }

  // Stop recording
  stopRecording() {
    if (this.mediaRecorder) {
      this.mediaRecorder.stop();
      this.isRecording = false;
    }
  }

  // Send the recorded audio to the backend for transcription
  sendAudioToBackend(audioBlob: Blob) {
    this.speechToTextService.transcribeSpeech(audioBlob).subscribe(
      (response) => {
        this.recognizedText = response.recognized_text;
      },
      (error) => {
        console.error('Error transcribing speech:', error);
      }
    );
  }
}

---------------



@app.post("/recognize-speech/")
async def recognize_speech(file: UploadFile = File(...)):
    try:
        # Generate a unique filename for the uploaded file
        file_name = str(uuid.uuid4()) + ".wav"
        file_path = os.path.join(audio_folder, file_name)

        # Use the 'with' statement to ensure the file is closed after writing
        with open(file_path, "wb") as f:
            # Write the contents of the uploaded file to the local file
            f.write(await file.read())

        # Ensure the audio file is in the correct format (16kHz, 16-bit PCM)
        audio = AudioSegment.from_file(file_path)
        audio = audio.set_channels(1)  # Mono
        audio = audio.set_sample_width(2)  # 16-bit PCM
        audio = audio.set_frame_rate(16000)  # Sample rate of 16kHz
        audio.export(file_path, format="wav")

        # Initialize speech configuration with Azure subscription key and region
        speech_config = SpeechConfig(subscription=subscription_key, region=region)
        
        # Create AudioConfig pointing to the saved audio file
        audio_config = AudioConfig(filename=file_path)
        
        # Create SpeechRecognizer object
        recognizer = SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
        
        # Start speech recognition asynchronously
        result = await recognizer.recognize_once_async()

        # Check if speech was recognized successfully
        if result.reason == result.Reason.RecognizedSpeech:
            return {"recognized_text": result.text}
        else:
            raise HTTPException(status_code=500, detail="Speech recognition failed")

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
